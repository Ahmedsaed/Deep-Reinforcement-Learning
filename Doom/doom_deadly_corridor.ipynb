{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import cv2\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config('Github repos\\\\ViZDoom\\\\scenarios\\\\deadly_corridor_s1.cfg')\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.identity(7, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -0.836029052734375\n",
      "reward: -0.5425567626953125\n",
      "reward: -1.5323333740234375\n",
      "reward: -1.8193511962890625\n",
      "reward: -1.227294921875\n",
      "reward: -7.463043212890625\n",
      "reward: -1.7415618896484375\n",
      "reward: 1.618743896484375\n",
      "reward: 7.362823486328125\n",
      "reward: 14.530548095703125\n",
      "reward: 13.094192504882812\n",
      "reward: 2.5609283447265625\n",
      "reward: -1.5659942626953125\n",
      "reward: -1.056365966796875\n",
      "reward: -7.742034912109375\n",
      "reward: -8.913650512695312\n",
      "reward: -7.0559234619140625\n",
      "reward: -6.350830078125\n",
      "reward: -2.054718017578125\n",
      "reward: 5.0536651611328125\n",
      "reward: 3.6894378662109375\n",
      "reward: 9.571929931640625\n",
      "reward: 10.176101684570312\n",
      "reward: -0.0865631103515625\n",
      "reward: -3.58282470703125\n",
      "reward: -2.35882568359375\n",
      "reward: -1.5912017822265625\n",
      "reward: -8.1837158203125\n",
      "reward: -16.364334106445312\n",
      "reward: -1.3274383544921875\n",
      "reward: 7.307342529296875\n",
      "reward: 1.552337646484375\n",
      "reward: 4.4232177734375\n",
      "reward: -0.393096923828125\n",
      "reward: -3.9991302490234375\n",
      "reward: -2.6975860595703125\n",
      "reward: -1.81964111328125\n",
      "reward: 5.8319244384765625\n",
      "reward: 7.0532073974609375\n",
      "reward: 0.952667236328125\n",
      "reward: 1.0392913818359375\n",
      "reward: 0.3038787841796875\n",
      "reward: -2.7681884765625\n",
      "reward: -3.3947906494140625\n",
      "reward: -9.322036743164062\n",
      "reward: -4.2502899169921875\n",
      "reward: 6.81683349609375\n",
      "reward: 3.3508453369140625\n",
      "reward: 3.095947265625\n",
      "reward: 1.69110107421875\n",
      "reward: 0.701507568359375\n",
      "reward: 0.47308349609375\n",
      "reward: -0.581024169921875\n",
      "reward: -1.78082275390625\n",
      "reward: -1.5983734130859375\n",
      "reward: -1.87060546875\n",
      "reward: -7.57440185546875\n",
      "reward: -2.4717559814453125\n",
      "reward: -0.2855377197265625\n",
      "reward: -0.0420074462890625\n",
      "reward: 7.1137542724609375\n",
      "reward: 8.476760864257812\n",
      "reward: 3.5010528564453125\n",
      "reward: 2.361419677734375\n",
      "reward: 1.592681884765625\n",
      "reward: -1.1460113525390625\n",
      "reward: -1.891143798828125\n",
      "reward: -1.2757110595703125\n",
      "reward: -0.860595703125\n",
      "reward: -0.580596923828125\n",
      "reward: 6.0271759033203125\n",
      "reward: 0.41253662109375\n",
      "reward: -1.915863037109375\n",
      "reward: 0.9271087646484375\n",
      "reward: 1.0425872802734375\n",
      "reward: 0.435211181640625\n",
      "reward: 3.4786529541015625\n",
      "reward: 7.0932769775390625\n",
      "reward: 12.42230224609375\n",
      "reward: 17.61114501953125\n",
      "reward: 15.057723999023438\n",
      "reward: 10.156585693359375\n",
      "reward: 6.8507232666015625\n",
      "reward: -2.328277587890625\n",
      "reward: -5.5772705078125\n",
      "reward: 4.7515869140625\n",
      "reward: 12.228515625\n",
      "reward: 3.3929290771484375\n",
      "reward: -1.3607635498046875\n",
      "reward: 6.031036376953125\n",
      "reward: 7.717132568359375\n",
      "reward: 5.205230712890625\n",
      "reward: -3.572174072265625\n",
      "reward: -4.379058837890625\n",
      "reward: -8.455917358398438\n",
      "reward: -9.423294067382812\n",
      "reward: -8.47723388671875\n",
      "reward: -6.348419189453125\n",
      "reward: -4.90997314453125\n",
      "reward: -3.6416015625\n",
      "reward: -2.5276641845703125\n",
      "reward: -7.7509002685546875\n",
      "reward: -8.3201904296875\n",
      "reward: -4.6550445556640625\n",
      "reward: -2.68865966796875\n",
      "reward: -1.5778656005859375\n",
      "reward: 5.88458251953125\n",
      "reward: 7.6183319091796875\n",
      "reward: 11.849136352539062\n",
      "reward: 18.226852416992188\n",
      "reward: 13.556930541992188\n",
      "reward: 1.355621337890625\n",
      "reward: -2.351593017578125\n",
      "reward: 0.6748199462890625\n",
      "reward: -0.6187591552734375\n",
      "reward: -8.315628051757812\n",
      "reward: -9.133148193359375\n",
      "reward: -6.1605377197265625\n",
      "reward: 2.958282470703125\n",
      "reward: 5.7309722900390625\n",
      "reward: 3.00823974609375\n",
      "reward: 7.1137542724609375\n",
      "reward: 8.533950805664062\n",
      "reward: 8.699874877929688\n",
      "reward: 1.2057342529296875\n",
      "reward: 4.1654052734375\n",
      "reward: 7.3526153564453125\n",
      "reward: 4.562286376953125\n",
      "reward: 3.474090576171875\n",
      "reward: 1.946197509765625\n",
      "reward: 0.03759765625\n",
      "reward: -0.9678192138671875\n",
      "reward: -0.1285552978515625\n",
      "reward: 0.352142333984375\n",
      "reward: -0.59857177734375\n",
      "reward: -1.67889404296875\n",
      "reward: -0.836090087890625\n",
      "reward: -0.29638671875\n",
      "reward: -0.0576019287109375\n",
      "reward: 0.0\n",
      "reward: -7.1103515625\n",
      "reward: -8.530059814453125\n",
      "reward: -5.753753662109375\n",
      "reward: -3.881103515625\n",
      "reward: -2.6179656982421875\n",
      "reward: -1.7659149169921875\n",
      "reward: 3.3949432373046875\n",
      "reward: -0.5692138671875\n",
      "reward: -8.41754150390625\n",
      "reward: -7.52032470703125\n",
      "reward: -0.4947052001953125\n",
      "reward: -3.8166046142578125\n",
      "reward: -5.4085235595703125\n",
      "reward: -8.860015869140625\n",
      "reward: -4.414520263671875\n",
      "reward: -0.8448486328125\n",
      "reward: -5.538421630859375\n",
      "reward: -10.858291625976562\n",
      "reward: -5.1464996337890625\n",
      "reward: -5.64947509765625\n",
      "reward: -6.219207763671875\n",
      "reward: -4.195068359375\n",
      "reward: -2.8297576904296875\n",
      "reward: -2.9522857666015625\n",
      "reward: -5.9501953125\n",
      "reward: -3.7894744873046875\n",
      "reward: -2.008331298828125\n",
      "reward: -1.354736328125\n",
      "reward: -0.9138641357421875\n",
      "reward: -0.616546630859375\n",
      "reward: 6.4831390380859375\n",
      "reward: 7.6895904541015625\n",
      "reward: -0.2916107177734375\n",
      "reward: -7.967437744140625\n",
      "reward: -7.3380889892578125\n",
      "reward: 2.820648193359375\n",
      "reward: 12.424560546875\n",
      "reward: 18.902557373046875\n",
      "reward: 23.27203369140625\n",
      "reward: 20.979446411132812\n",
      "reward: 15.022171020507812\n",
      "reward: 8.473220825195312\n",
      "reward: 4.843841552734375\n",
      "reward: 3.2671661376953125\n",
      "reward: 2.2036590576171875\n",
      "reward: 8.385406494140625\n",
      "reward: 10.938186645507812\n",
      "reward: 6.5898590087890625\n",
      "reward: 10.472549438476562\n",
      "reward: 10.686767578125\n",
      "reward: 5.1670684814453125\n",
      "reward: -4.7384033203125\n",
      "reward: -0.0456085205078125\n",
      "reward: 9.986175537109375\n",
      "reward: 3.9868621826171875\n",
      "reward: 2.6890869140625\n",
      "reward: 1.5048065185546875\n",
      "reward: 0.9524688720703125\n",
      "reward: -6.3870697021484375\n",
      "reward: -6.956390380859375\n",
      "reward: -4.1444549560546875\n",
      "reward: -9.905929565429688\n",
      "reward: -7.9883575439453125\n",
      "reward: -0.00830078125\n",
      "reward: 0.0\n",
      "reward: -0.5863494873046875\n",
      "reward: -0.6931304931640625\n",
      "reward: -0.467620849609375\n",
      "reward: -0.315521240234375\n",
      "reward: -0.0613250732421875\n",
      "reward: -6.9936370849609375\n",
      "reward: -8.389999389648438\n",
      "reward: 1.334136962890625\n",
      "reward: 4.57232666015625\n",
      "reward: 9.973434448242188\n",
      "reward: 1.52191162109375\n",
      "reward: -0.069244384765625\n",
      "reward: -0.752105712890625\n",
      "reward: -7.775146484375\n",
      "reward: -16.073715209960938\n",
      "reward: -14.57080078125\n",
      "reward: -8.583541870117188\n",
      "reward: -5.1361541748046875\n",
      "reward: -3.4645233154296875\n",
      "reward: -2.3369903564453125\n",
      "reward: 5.507049560546875\n",
      "reward: 6.8118133544921875\n",
      "reward: 4.2677001953125\n",
      "reward: 1.7975921630859375\n",
      "reward: 4.13800048828125\n",
      "reward: 3.661834716796875\n",
      "reward: 2.186553955078125\n",
      "reward: 1.5740966796875\n",
      "reward: 1.3706512451171875\n",
      "reward: -5.74053955078125\n",
      "reward: -11.48309326171875\n",
      "reward: -10.745758056640625\n",
      "reward: -7.12677001953125\n",
      "reward: -5.4170074462890625\n",
      "reward: -8.087005615234375\n",
      "reward: -6.7281494140625\n",
      "reward: -4.7478790283203125\n",
      "reward: -3.31268310546875\n",
      "reward: -2.8570098876953125\n",
      "reward: -9.337677001953125\n",
      "reward: -9.39605712890625\n",
      "reward: -6.0111236572265625\n",
      "reward: -4.054718017578125\n",
      "reward: -2.7350616455078125\n",
      "reward: -8.742843627929688\n",
      "reward: -0.7942657470703125\n",
      "reward: 6.564453125\n",
      "reward: 1.0559234619140625\n",
      "reward: -3.430023193359375\n",
      "reward: 4.569427490234375\n",
      "reward: 6.81072998046875\n",
      "reward: 4.1750946044921875\n",
      "reward: -4.5038604736328125\n",
      "reward: -6.766510009765625\n",
      "reward: -4.564239501953125\n",
      "reward: -3.3050384521484375\n",
      "reward: -3.4111328125\n",
      "reward: -2.6278533935546875\n",
      "reward: -1.9821319580078125\n",
      "reward: -1.656646728515625\n",
      "reward: -5.26727294921875\n",
      "reward: -4.0545806884765625\n",
      "reward: -1.4466552734375\n",
      "reward: 5.6266021728515625\n",
      "reward: 6.208038330078125\n",
      "reward: 3.9679412841796875\n",
      "reward: 2.7712860107421875\n",
      "reward: 5.3644256591796875\n",
      "reward: -0.774627685546875\n",
      "reward: -4.256500244140625\n",
      "reward: -9.900588989257812\n",
      "reward: -8.943374633789062\n",
      "reward: 7.029266357421875\n",
      "reward: 8.432601928710938\n",
      "reward: 3.85467529296875\n",
      "reward: -6.8463897705078125\n",
      "reward: -8.30950927734375\n",
      "reward: 1.2376708984375\n",
      "reward: 6.2897186279296875\n",
      "reward: 5.2200775146484375\n",
      "reward: 3.5209808349609375\n",
      "reward: -4.6545562744140625\n",
      "reward: -6.831085205078125\n",
      "reward: -4.6077728271484375\n",
      "reward: -4.39410400390625\n",
      "reward: -0.0171051025390625\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.2498626708984375\n",
      "reward: 1.49932861328125\n",
      "reward: 1.01123046875\n",
      "reward: 5.972015380859375\n",
      "reward: 4.260284423828125\n",
      "reward: -5.159271240234375\n",
      "reward: -6.789947509765625\n",
      "reward: 2.7397003173828125\n",
      "reward: 12.676132202148438\n",
      "reward: 19.378433227539062\n",
      "reward: 23.89923095703125\n",
      "reward: 26.9486083984375\n",
      "reward: 14.805572509765625\n",
      "reward: 6.2580108642578125\n",
      "reward: 4.2210540771484375\n",
      "reward: 2.8470916748046875\n",
      "reward: 1.9203338623046875\n",
      "reward: 1.295135498046875\n",
      "reward: 0.873504638671875\n",
      "reward: 0.589111328125\n",
      "reward: -1.45953369140625\n",
      "reward: -101.05999755859375\n",
      "Result: 1.982269287109375\n"
     ]
    }
   ],
   "source": [
    "# Loop through episodes \n",
    "episodes = 1 \n",
    "for episode in range(episodes): \n",
    "    # Create a new episode or game \n",
    "    game.new_episode()\n",
    "    # Check the game isn't done \n",
    "    while not game.is_episode_finished(): \n",
    "        # Get the game state \n",
    "        state = game.get_state()\n",
    "        # Get the game image \n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables - ammo\n",
    "        info = state.game_variables\n",
    "        # Take an action\n",
    "        reward = game.make_action(random.choice(actions),4)\n",
    "        # Print reward \n",
    "        print('reward:', reward) \n",
    "        time.sleep(0.02)\n",
    "    print('Result:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup VizDoom OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env): \n",
    "    # Function that is called when we start the env\n",
    "    def __init__(self, render=False, config='1'): \n",
    "        # Inherit from Env\n",
    "        super().__init__()\n",
    "        # Setup the game \n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(f'Github repos\\\\ViZDoom\\\\scenarios\\\\deadly_corridor_s{config}.cfg')\n",
    "        \n",
    "        # Render frame logic\n",
    "        self.game.set_window_visible(render)\n",
    "        \n",
    "        # Start the game \n",
    "        self.game.init()\n",
    "        \n",
    "        # Create the action space and observation space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(100,160,1), dtype=np.uint8) \n",
    "        self.action_space = Discrete(7)\n",
    "        \n",
    "        # Game variables: HEALTH DAMAGE_TAKEN HITCOUNT SELECTED_WEAPON_AMMO\n",
    "        self.damage_taken = 0\n",
    "        self.hitcount = 0\n",
    "        self.ammo = 52 ## CHANGED\n",
    "        \n",
    "        \n",
    "    # This is how we take a step in the environment\n",
    "    def step(self, action):\n",
    "        # Specify action and take step \n",
    "        actions = np.identity(7)\n",
    "        movement_reward = self.game.make_action(actions[action], 4) \n",
    "        \n",
    "        reward = 0 \n",
    "        # Get all the other stuff we need to return \n",
    "        if self.game.get_state(): \n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            \n",
    "            # Reward shaping\n",
    "            game_variables = self.game.get_state().game_variables\n",
    "            health, damage_taken, hitcount, ammo = game_variables\n",
    "            \n",
    "            # Calculate reward deltas\n",
    "            damage_taken_delta = -damage_taken + self.damage_taken\n",
    "            self.damage_taken = damage_taken\n",
    "            hitcount_delta = hitcount - self.hitcount\n",
    "            self.hitcount = hitcount\n",
    "            ammo_delta = ammo - self.ammo\n",
    "            self.ammo = ammo\n",
    "            \n",
    "            reward = movement_reward + damage_taken_delta*10 + hitcount_delta*200  + ammo_delta*5 \n",
    "            info = ammo\n",
    "        else: \n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0 \n",
    "        \n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, info \n",
    "    \n",
    "    # Define how to render the game or environment \n",
    "    def render(): \n",
    "        pass\n",
    "    \n",
    "    # What happens when we start a new game \n",
    "    def reset(self): \n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        return self.grayscale(state)\n",
    "    \n",
    "    # Grayscale the game frame and resize it \n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160,100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "    \n",
    "    # Call to close down the game\n",
    "    def close(self): \n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Deadly Corridor using curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_deadly_corridor'\n",
    "LOG_DIR = './logs/log_deadly_corridor'\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "env = VizDoomGym(render=False, config='1')\n",
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.00001, n_steps=8192, clip_range=.1, gamma=.95, gae_lambda=.9)\n",
    "# model.load(\".\\\\train\\\\train_deadly_corridor\\\\best_model_250000.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skill Level: 1\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_7\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | 97.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 17       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 463      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 162          |\n",
      "|    ep_rew_mean          | 143          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1256         |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025107607 |\n",
      "|    clip_fraction        | 0.098        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | -1.61e-05    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 8.65e+03     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    value_loss           | 7.78e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 140          |\n",
      "|    ep_rew_mean          | 145          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 1877         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030214721 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0135       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.09e+03     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0046      |\n",
      "|    value_loss           | 9.22e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 142          |\n",
      "|    ep_rew_mean          | 154          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2461         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026766867 |\n",
      "|    clip_fraction        | 0.136        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | 0.0382       |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 2.73e+03     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 9.46e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 111          |\n",
      "|    ep_rew_mean          | 228          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3022         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024371338 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.92        |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 2.16e+03     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00242     |\n",
      "|    value_loss           | 8.77e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 115         |\n",
      "|    ep_rew_mean          | 252         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3787        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003077298 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 3.84e+03    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 1.17e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 109          |\n",
      "|    ep_rew_mean          | 305          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 4493         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026199687 |\n",
      "|    clip_fraction        | 0.17         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.89        |\n",
      "|    explained_variance   | 0.164        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.27e+04     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 9.67e+03     |\n",
      "------------------------------------------\n",
      "Skill Level: 2\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_8\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43.8     |\n",
      "|    ep_rew_mean     | 152      |\n",
      "| time/              |          |\n",
      "|    fps             | 22       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 371      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 47           |\n",
      "|    ep_rew_mean          | 233          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 16           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 988          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028275503 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.86        |\n",
      "|    explained_variance   | 0.196        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.52e+04     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000816    |\n",
      "|    value_loss           | 2.7e+04      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 56.2        |\n",
      "|    ep_rew_mean          | 175         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1627        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002183336 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.74e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00102     |\n",
      "|    value_loss           | 2.8e+04     |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 51.3      |\n",
      "|    ep_rew_mean          | 213       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 14        |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 2290      |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0027598 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -1.85     |\n",
      "|    explained_variance   | 0.301     |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 1.06e+04  |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 0.000114  |\n",
      "|    value_loss           | 2.32e+04  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 46.8         |\n",
      "|    ep_rew_mean          | 209          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3048         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031568962 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.83        |\n",
      "|    explained_variance   | 0.309        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.21e+04     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -3.91e-06    |\n",
      "|    value_loss           | 2.38e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 45.9         |\n",
      "|    ep_rew_mean          | 228          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 3762         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038026655 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.79        |\n",
      "|    explained_variance   | 0.367        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.41e+04     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -1.87e-05    |\n",
      "|    value_loss           | 2.43e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 52.2         |\n",
      "|    ep_rew_mean          | 269          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 4480         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035276832 |\n",
      "|    clip_fraction        | 0.206        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.79        |\n",
      "|    explained_variance   | 0.393        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.07e+04     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | 0.00077      |\n",
      "|    value_loss           | 2.41e+04     |\n",
      "------------------------------------------\n",
      "Skill Level: 3\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_9\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.6     |\n",
      "|    ep_rew_mean     | 261      |\n",
      "| time/              |          |\n",
      "|    fps             | 17       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 476      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 50.5         |\n",
      "|    ep_rew_mean          | 283          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1243         |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036312342 |\n",
      "|    clip_fraction        | 0.201        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.76        |\n",
      "|    explained_variance   | 0.418        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 8.79e+03     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | 0.00102      |\n",
      "|    value_loss           | 2.18e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 59.9        |\n",
      "|    ep_rew_mean          | 298         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2027        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003181993 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 7.44e+03    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.00114     |\n",
      "|    value_loss           | 2.07e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 54.9        |\n",
      "|    ep_rew_mean          | 288         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 11          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2782        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003931732 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.404       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.42e+04    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.000322    |\n",
      "|    value_loss           | 1.78e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.6         |\n",
      "|    ep_rew_mean          | 311          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 11           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3575         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039281556 |\n",
      "|    clip_fraction        | 0.229        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | 0.399        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.03e+04     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 0.000918     |\n",
      "|    value_loss           | 1.92e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 56.3         |\n",
      "|    ep_rew_mean          | 305          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 11           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 4391         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035390947 |\n",
      "|    clip_fraction        | 0.206        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | 0.379        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 7.97e+03     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | 0.000455     |\n",
      "|    value_loss           | 1.94e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 64.1         |\n",
      "|    ep_rew_mean          | 329          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 5244         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039129807 |\n",
      "|    clip_fraction        | 0.265        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.69        |\n",
      "|    explained_variance   | 0.383        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 8.98e+03     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | 0.000868     |\n",
      "|    value_loss           | 1.91e+04     |\n",
      "------------------------------------------\n",
      "Skill Level: 4\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_10\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 65       |\n",
      "|    ep_rew_mean     | 347      |\n",
      "| time/              |          |\n",
      "|    fps             | 16       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 499      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60.8         |\n",
      "|    ep_rew_mean          | 374          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1325         |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041405708 |\n",
      "|    clip_fraction        | 0.241        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.68        |\n",
      "|    explained_variance   | 0.428        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.63e+03     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | 6.08e-05     |\n",
      "|    value_loss           | 1.58e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 55.2         |\n",
      "|    ep_rew_mean          | 375          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2023         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046613673 |\n",
      "|    clip_fraction        | 0.249        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 0.434        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.13e+04     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | 0.00139      |\n",
      "|    value_loss           | 1.81e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 52.7        |\n",
      "|    ep_rew_mean          | 410         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2602        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005479169 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 8.55e+03    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.0016      |\n",
      "|    value_loss           | 2.03e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 50.7         |\n",
      "|    ep_rew_mean          | 398          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 12           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3181         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041706124 |\n",
      "|    clip_fraction        | 0.233        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.431        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.01e+04     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | 0.00235      |\n",
      "|    value_loss           | 2.25e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 49.2        |\n",
      "|    ep_rew_mean          | 419         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3762        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004377611 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.08e+04    |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | 0.00182     |\n",
      "|    value_loss           | 2.18e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 45.9         |\n",
      "|    ep_rew_mean          | 390          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 4347         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037047467 |\n",
      "|    clip_fraction        | 0.238        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.478        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.27e+04     |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | 0.00291      |\n",
      "|    value_loss           | 2.4e+04      |\n",
      "------------------------------------------\n",
      "Skill Level: 5\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_11\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.2     |\n",
      "|    ep_rew_mean     | 263      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 26.3      |\n",
      "|    ep_rew_mean          | 246       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 18        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 874       |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0485897 |\n",
      "|    clip_fraction        | 0.473     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -1.31     |\n",
      "|    explained_variance   | -0.173    |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 1.65e+04  |\n",
      "|    n_updates            | 290       |\n",
      "|    policy_gradient_loss | 0.0399    |\n",
      "|    value_loss           | 2.81e+04  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 26.2       |\n",
      "|    ep_rew_mean          | 247        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 16         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 1464       |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00419924 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -1.29      |\n",
      "|    explained_variance   | 0.306      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 9.99e+03   |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | 0.00259    |\n",
      "|    value_loss           | 2.77e+04   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 25.6         |\n",
      "|    ep_rew_mean          | 246          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 16           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2047         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041081943 |\n",
      "|    clip_fraction        | 0.169        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.38         |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.6e+04      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | 0.0023       |\n",
      "|    value_loss           | 2.54e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 23.4        |\n",
      "|    ep_rew_mean          | 223         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2644        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004096619 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.21e+04    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.000303    |\n",
      "|    value_loss           | 2.28e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 25.1         |\n",
      "|    ep_rew_mean          | 261          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 3239         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074049025 |\n",
      "|    clip_fraction        | 0.175        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.425        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.18e+04     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | 0.00129      |\n",
      "|    value_loss           | 2.48e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 26.5         |\n",
      "|    ep_rew_mean          | 281          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 3829         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028536182 |\n",
      "|    clip_fraction        | 0.154        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 9.41e+03     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | 0.000919     |\n",
      "|    value_loss           | 2.16e+04     |\n",
      "------------------------------------------\n",
      "Skill Level: 1\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_12\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 52.8     |\n",
      "|    ep_rew_mean     | 631      |\n",
      "| time/              |          |\n",
      "|    fps             | 29       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 278      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 52.2       |\n",
      "|    ep_rew_mean          | 607        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 19         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 857        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02133408 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -1.27      |\n",
      "|    explained_variance   | -0.000581  |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 4.84e+03   |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | 0.0369     |\n",
      "|    value_loss           | 1.62e+04   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 51.2         |\n",
      "|    ep_rew_mean          | 632          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 17           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 1433         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046155676 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | 0.373        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.11e+04     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | 0.00199      |\n",
      "|    value_loss           | 1.78e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 49.2         |\n",
      "|    ep_rew_mean          | 708          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 16           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2012         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039741127 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0.447        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 8.96e+03     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | 0.00181      |\n",
      "|    value_loss           | 1.76e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 45.9        |\n",
      "|    ep_rew_mean          | 642         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2588        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004212557 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.44e+04    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.00541     |\n",
      "|    value_loss           | 2.02e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 47.4         |\n",
      "|    ep_rew_mean          | 659          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 3154         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036432997 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.473        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 8.03e+03     |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | 0.00159      |\n",
      "|    value_loss           | 1.98e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 46           |\n",
      "|    ep_rew_mean          | 675          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 3725         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030399342 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.996       |\n",
      "|    explained_variance   | 0.508        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.21e+04     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | 0.00186      |\n",
      "|    value_loss           | 1.94e+04     |\n",
      "------------------------------------------\n",
      "Skill Level: 2\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_13\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.9     |\n",
      "|    ep_rew_mean     | 507      |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 266      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.6        |\n",
      "|    ep_rew_mean          | 510         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 19          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 843         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008746928 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.893      |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.6e+04     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.00285     |\n",
      "|    value_loss           | 3.46e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37           |\n",
      "|    ep_rew_mean          | 518          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 17           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 1421         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045438414 |\n",
      "|    clip_fraction        | 0.159        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.893       |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.45e+04     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | 0.00274      |\n",
      "|    value_loss           | 3.24e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37          |\n",
      "|    ep_rew_mean          | 517         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1997        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008082985 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.87       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.67e+04    |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.00444     |\n",
      "|    value_loss           | 3.06e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.4        |\n",
      "|    ep_rew_mean          | 506         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2576        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003204688 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.837      |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.49e+04    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00297     |\n",
      "|    value_loss           | 3.19e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.8         |\n",
      "|    ep_rew_mean          | 532          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 3151         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037115326 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.832       |\n",
      "|    explained_variance   | 0.487        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.42e+04     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | 0.00424      |\n",
      "|    value_loss           | 2.99e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 35.4        |\n",
      "|    ep_rew_mean          | 510         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 3728        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003804487 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.8        |\n",
      "|    explained_variance   | 0.476       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.04e+04    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.00414     |\n",
      "|    value_loss           | 2.9e+04     |\n",
      "-----------------------------------------\n",
      "Skill Level: 3\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_14\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 35.3     |\n",
      "|    ep_rew_mean     | 508      |\n",
      "| time/              |          |\n",
      "|    fps             | 31       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.4         |\n",
      "|    ep_rew_mean          | 517          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 19           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 842          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037219487 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.736       |\n",
      "|    explained_variance   | 0.504        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.98e+04     |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | 0.00263      |\n",
      "|    value_loss           | 2.94e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 36.3       |\n",
      "|    ep_rew_mean          | 511        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 17         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 1431       |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00548675 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -0.725     |\n",
      "|    explained_variance   | 0.509      |\n",
      "|    learning_rate        | 1e-05      |\n",
      "|    loss                 | 1.34e+04   |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | 0.00399    |\n",
      "|    value_loss           | 2.98e+04   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37           |\n",
      "|    ep_rew_mean          | 509          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 16           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2014         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033053807 |\n",
      "|    clip_fraction        | 0.151        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.715       |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.32e+04     |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | 0.00361      |\n",
      "|    value_loss           | 2.7e+04      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37.7         |\n",
      "|    ep_rew_mean          | 540          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 2598         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036229328 |\n",
      "|    clip_fraction        | 0.157        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.726       |\n",
      "|    explained_variance   | 0.574        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.47e+04     |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | 0.0051       |\n",
      "|    value_loss           | 2.54e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 38.3        |\n",
      "|    ep_rew_mean          | 507         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 3186        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004147138 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.585       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 7.39e+03    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | 0.00265     |\n",
      "|    value_loss           | 2.53e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.5         |\n",
      "|    ep_rew_mean          | 497          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 3783         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063767536 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.72        |\n",
      "|    explained_variance   | 0.611        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.33e+04     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | 0.00437      |\n",
      "|    value_loss           | 2.43e+04     |\n",
      "------------------------------------------\n",
      "Skill Level: 4\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_15\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.2     |\n",
      "|    ep_rew_mean     | 520      |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 266      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.6         |\n",
      "|    ep_rew_mean          | 512          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 863          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032742126 |\n",
      "|    clip_fraction        | 0.151        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.669       |\n",
      "|    explained_variance   | 0.638        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.93e+04     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | 0.00385      |\n",
      "|    value_loss           | 2.41e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37.1         |\n",
      "|    ep_rew_mean          | 499          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 16           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 1455         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035132575 |\n",
      "|    clip_fraction        | 0.142        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.66        |\n",
      "|    explained_variance   | 0.62         |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.17e+04     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | 0.00513      |\n",
      "|    value_loss           | 2.41e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.3        |\n",
      "|    ep_rew_mean          | 516         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2046        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004269786 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.642      |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.05e+04    |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | 0.00322     |\n",
      "|    value_loss           | 2.38e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.8         |\n",
      "|    ep_rew_mean          | 489          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 2643         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040610544 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.634       |\n",
      "|    explained_variance   | 0.655        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.49e+04     |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | 0.00425      |\n",
      "|    value_loss           | 2.35e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37.8         |\n",
      "|    ep_rew_mean          | 521          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 3236         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044258116 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.625       |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.07e+04     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | 0.00535      |\n",
      "|    value_loss           | 2.27e+04     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.1        |\n",
      "|    ep_rew_mean          | 508         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 3829        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004409185 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.4e+04     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | 0.00519     |\n",
      "|    value_loss           | 2.28e+04    |\n",
      "-----------------------------------------\n",
      "Skill Level: 5\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_16\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.4     |\n",
      "|    ep_rew_mean     | 374      |\n",
      "| time/              |          |\n",
      "|    fps             | 30       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 266      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 17.2      |\n",
      "|    ep_rew_mean          | 194       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 18        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 867       |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0742722 |\n",
      "|    clip_fraction        | 0.624     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -0.331    |\n",
      "|    explained_variance   | -0.24     |\n",
      "|    learning_rate        | 1e-05     |\n",
      "|    loss                 | 1.41e+04  |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | 0.179     |\n",
      "|    value_loss           | 2.32e+04  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18          |\n",
      "|    ep_rew_mean          | 208         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1470        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037946276 |\n",
      "|    clip_fraction        | 0.0991      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.266      |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 2.31e+04    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    value_loss           | 3.58e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.9        |\n",
      "|    ep_rew_mean          | 219         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 15          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2076        |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004207487 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 1.69e+04    |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.00229     |\n",
      "|    value_loss           | 3.45e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 16.9         |\n",
      "|    ep_rew_mean          | 228          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 2685         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025918102 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.477        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.21e+04     |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | 0.00119      |\n",
      "|    value_loss           | 3.3e+04      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 17.3         |\n",
      "|    ep_rew_mean          | 233          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 3288         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022439153 |\n",
      "|    clip_fraction        | 0.0758       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.465        |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.53e+04     |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | 0.00315      |\n",
      "|    value_loss           | 3.22e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 16           |\n",
      "|    ep_rew_mean          | 224          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 14           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 3898         |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028442484 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.226       |\n",
      "|    explained_variance   | 0.48         |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 1.22e+04     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | 0.000836     |\n",
      "|    value_loss           | 3.09e+04     |\n",
      "------------------------------------------\n",
      "Skill Level: 1\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/log_deadly_corridor\\PPO_17\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46       |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 32       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 45           |\n",
      "|    ep_rew_mean          | 1.15e+03     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 19           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 833          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034415333 |\n",
      "|    clip_fraction        | 0.057        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -0.189       |\n",
      "|    explained_variance   | -0.0842      |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 9.32e+03     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | 0.000857     |\n",
      "|    value_loss           | 2.21e+04     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ahmed\\Documents\\Projects\\Deep Reinforcement Learning\\Doom\\doom_deadly_corridor.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ahmed/Documents/Projects/Deep%20Reinforcement%20Learning/Doom/doom_deadly_corridor.ipynb#ch0000016?line=3'>4</a>\u001b[0m env \u001b[39m=\u001b[39m VizDoomGym(render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, config\u001b[39m=\u001b[39m\u001b[39mstr\u001b[39m(i))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ahmed/Documents/Projects/Deep%20Reinforcement%20Learning/Doom/doom_deadly_corridor.ipynb#ch0000016?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mset_env(env)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ahmed/Documents/Projects/Deep%20Reinforcement%20Learning/Doom/doom_deadly_corridor.ipynb#ch0000016?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m50000\u001b[39;49m, callback\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[1;32mc:\\Python venv\\deep-RL-pytorch\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:299\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=285'>286</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=286'>287</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=287'>288</a>\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=295'>296</a>\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=296'>297</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPPO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(PPO, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=299'>300</a>\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=300'>301</a>\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=301'>302</a>\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=302'>303</a>\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=303'>304</a>\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=304'>305</a>\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=305'>306</a>\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=306'>307</a>\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=307'>308</a>\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=308'>309</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python venv\\deep-RL-pytorch\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:270\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=266'>267</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=267'>268</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[1;32m--> <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=269'>270</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=271'>272</a>\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/on_policy_algorithm.py?line=273'>274</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python venv\\deep-RL-pytorch\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:189\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=186'>187</a>\u001b[0m approx_kl_divs \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=187'>188</a>\u001b[0m \u001b[39m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=188'>189</a>\u001b[0m \u001b[39mfor\u001b[39;00m rollout_data \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrollout_buffer\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=189'>190</a>\u001b[0m     actions \u001b[39m=\u001b[39m rollout_data\u001b[39m.\u001b[39mactions\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=190'>191</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mDiscrete):\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/ppo/ppo.py?line=191'>192</a>\u001b[0m         \u001b[39m# Convert discrete action from float to long\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python venv\\deep-RL-pytorch\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:463\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=460'>461</a>\u001b[0m start_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=461'>462</a>\u001b[0m \u001b[39mwhile\u001b[39;00m start_idx \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_envs:\n\u001b[1;32m--> <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=462'>463</a>\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_samples(indices[start_idx : start_idx \u001b[39m+\u001b[39;49m batch_size])\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=463'>464</a>\u001b[0m     start_idx \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_size\n",
      "File \u001b[1;32mc:\\Python venv\\deep-RL-pytorch\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:468\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[1;34m(self, batch_inds, env)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=465'>466</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_samples\u001b[39m(\u001b[39mself\u001b[39m, batch_inds: np\u001b[39m.\u001b[39mndarray, env: Optional[VecNormalize] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RolloutBufferSamples:\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=466'>467</a>\u001b[0m     data \u001b[39m=\u001b[39m (\n\u001b[1;32m--> <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=467'>468</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservations[batch_inds],\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=468'>469</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[batch_inds],\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=469'>470</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=470'>471</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_probs[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=471'>472</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvantages[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=472'>473</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturns[batch_inds]\u001b[39m.\u001b[39mflatten(),\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=473'>474</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Python%20venv/deep-RL-pytorch/lib/site-packages/stable_baselines3/common/buffers.py?line=474'>475</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m RolloutBufferSamples(\u001b[39m*\u001b[39m\u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_torch, data)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    for i in range(1, 6):\n",
    "        print(f\"Skill Level: {i}\")\n",
    "        env = VizDoomGym(render=False, config=str(i))\n",
    "        model.set_env(env)\n",
    "        model.learn(total_timesteps=50000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('train\\\\train_deadly_corridor\\\\best_model_100000.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True, config=\"5\")\n",
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "mean_reward\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca9f952ef321d99a2ab24cb17ecd9558e5828ac6a303314df963ea7fdd58e81b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('deep-RL-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
